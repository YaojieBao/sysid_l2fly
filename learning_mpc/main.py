#!/usr/bin/env python
""" main.py:
Learns dynamics of plant and uses MPC to control it.

Based on material presented at Berkeley's Deep Reinforcement Learning
course (Fall 2017):  
http://rll.berkeley.edu/deeprlcourse/

VERSION CONTROL
0.0.1 (May 23, 2018): initial release
"""

__author__ = "Vinicius Guimaraes Goecks"
__version__ = "0.0.1"
__status__ = "Prototype"
__date__ = "May 23, 2018"

# import
import numpy as np
import tensorflow as tf
import gym
from dynamics import NNDynamicsModel
from controllers import MPCcontroller, RandomController
from cost_functions import aircraft_cost_fn, pendulum_cost_fn
from cost_functions import trajectory_cost_fn
import time
import logz
import os
import copy
import matplotlib.pyplot as plt

from aircraft_env import AircraftEnv

import inspect

def sample(env, 
           controller, 
           num_paths=10, 
           horizon=1000, 
           render=False,
           verbose=False):
    """
    Takes in an environment, a controller, and returns rollouts by running on
    the env. Each path can have elements for observations, next_observations,
    rewards, returns, actions, etc.
    """
    
    paths = {
        "observations":[], 
        "next_observations":[], 
        "rewards":[], 
        "actions":[], 
        "ep_lens":[],
        "acc_rewards":[]
    }

    for i in range(num_paths):
        animate_this_rollout = render and (i%10 == 0)
        print("Sample Path {} / {}".format(i, num_paths))
        ob = env.reset()
        ep_len = 0
        while ep_len < horizon:
            if animate_this_rollout:
                env.render()
                time.sleep(0.05)
            
            paths["observations"].append(ob)
            act = controller.get_action(ob)
            ob, rew, done, _ = env.step(act)
            
            paths["actions"].append(act)
            paths["next_observations"].append(ob)
            paths["rewards"].append(rew)

            ep_len += 1
            if done: break

        paths["ep_lens"].append(ep_len)
        paths["acc_rewards"].append(sum(paths["rewards"][-ep_len:]))

    if verbose:
        print("************* New Sample *************")
        returns = paths["acc_rewards"]
        ep_lengths = paths["ep_lens"]
        print("AverageReturn", np.mean(returns))
        print("StdReturn", np.std(returns))
        print("MaxReturn", np.max(returns))
        print("MinReturn", np.min(returns))
        print("EpLenMean", np.mean(ep_lengths))
        print("EpLenStd", np.std(ep_lengths))

    for key in paths.keys():
        paths[key] = np.array(paths[key])

    return paths

# Utility to compute cost a path for a given cost function
def path_cost(cost_fn, path):
    costs = []
    acc = 0
    for i in path["ep_lens"]:
        acc_n = acc + i
        costs.append(trajectory_cost_fn(
            cost_fn,
            path['observations'][acc:acc_n],
            path['actions'][acc:acc_n],
            path['next_observations'][acc:acc_n]))
        acc = acc_n
    return costs

def compute_normalization(data):
    """
    Takes in a dataset and compute the means, and stds.
    Return 6 elements: mean of s_t, std of s_t, mean of (s_t+1 - s_t),
    std of (s_t+1 - s_t), mean of actions, std of actions
    """
    return (np.mean(data, axis=0), np.std(data, axis=0))


def plot_comparison(env, dyn_model):
    """
    Generate plots comparing the behavior of the model predictions for each
    element of the state to the actual ground truth, using randomly sampled
    actions. 
    """
    pass


def train(env, 
         cost_fn,
         exp_name='test',
         logdir=None,
         render=False,
         learning_rate=1e-3,
         onpol_iters=10,
         dynamics_iters=60,
         batch_size=512,
         num_paths_random=10, 
         num_paths_onpol=10, 
         num_simulated_paths=10000,
         env_horizon=1000, 
         mpc_horizon=15,
         n_layers=2,
         size=500,
         activation=tf.nn.relu,
         output_activation=None
         ):

    """
    Arg:
        onpol_iters: Number of iterations of onpolicy aggregation for the loop
                     to run. 

        dynamics_iters: Number of iterations of training for the dynamics model
        |_              which happen per iteration of the aggregation loop.

        batch_size: Batch size for dynamics training.

        num_paths_random: Number of paths/trajectories/rollouts generated 
        |                 by a random agent. We use these to train our 
        |_                initial dynamics model.
    
        num_paths_onpol: Number of paths to collect at each iteration of
        |_               aggregation, using the MPC policy.

        num_simulated_paths: How many fictitious rollouts the MPC policy
        |                    should generate each time it is asked for an
        |_                   action.

        env_horizon: Number of timesteps in each path.

        mpc_horizon: The MPC policy generates actions by imagining 
        |            fictitious rollouts, and picking the first action
        |            of the best fictitious rollout. This argument is
        |            how many timesteps should be in each fictitious
        |_           rollout.

        n_layers/size/activations: Neural network architecture arguments. 
    """
    # Configure output directory for logging
    logz.configure_output_dir(logdir)

    # Log experimental parameters
    args = inspect.getargspec(train)[0]
    locals_ = locals()
    locals_['cost_fn'] = 'cost_fn'
    locals_['activation'] = 'activation'
    locals_['env'] = 'env'
    params = {k: locals_[k] if k in locals_ else None for k in args}
    logz.save_params(params)

    #========================================================
    # 
    # First, we need a lot of data generated by a random
    # agent, with which we'll begin to train our dynamics
    # model.

    random_controller = RandomController(env)
    
    paths = sample(
        env=env,
        controller=random_controller, 
        num_paths=num_paths_random,
        horizon=env_horizon,
        verbose=False)

    #========================================================
    # 
    # The random data will be used to get statistics (mean
    # and std) for the observations, actions, and deltas
    # (where deltas are o_{t+1} - o_t). These will be used
    # for normalizing inputs and denormalizing outputs
    # from the dynamics network. 
    # 
    normalization = {
        "observations": compute_normalization(paths["observations"]),
        "actions": compute_normalization(paths["actions"]), 
        "deltas": compute_normalization(
            paths["next_observations"] - paths["observations"]) 
    }

    #========================================================
    # 
    # Build dynamics model and MPC controllers.
    # 
    sess = tf.Session()

    dyn_model = NNDynamicsModel(env=env, 
                                n_layers=n_layers, 
                                size=size, 
                                activation=activation, 
                                output_activation=output_activation, 
                                normalization=normalization,
                                batch_size=batch_size,
                                iterations=dynamics_iters,
                                learning_rate=learning_rate,
                                sess=sess)

    mpc_controller = MPCcontroller(env=env, 
                                   dyn_model=dyn_model, 
                                   horizon=mpc_horizon, 
                                   cost_fn=cost_fn, 
                                   num_simulated_paths=num_simulated_paths)


    #========================================================
    # 
    # Tensorflow session building.
    # 
    sess.__enter__()
    tf.global_variables_initializer().run()

    #========================================================
    # 
    # Take multiple iterations of onpolicy aggregation at each iteration
    # refitting the dynamics model to current dataset and then taking onpolicy
    # samples and aggregating to the dataset. 
    # TODO: implement mixing ratio for new and old data as described in
    # https://arxiv.org/abs/1708.02596
    # 
    for itr in range(onpol_iters):

        shuffle_indexes = np.random.permutation(paths["observations"].shape[0])
        for key in ['observations', 'actions', 'next_observations', 'rewards']:
            paths[key] = paths[key][shuffle_indexes]

        dyn_model.fit(paths)

        newpaths = sample(
            env=env,
            controller=mpc_controller, 
            num_paths=num_paths_onpol,
            horizon=env_horizon,
            verbose=False)

        # LOGGING
        # Statistics for performance of MPC policy using
        # our learned dynamics model
        costs = path_cost(cost_fn, newpaths)            
        returns = newpaths["acc_rewards"]

        logz.log_tabular('Iteration', itr)
        # In terms of cost function which your MPC controller uses to plan
        logz.log_tabular('AverageCost', np.mean(costs))
        logz.log_tabular('StdCost', np.std(costs))
        logz.log_tabular('MinimumCost', np.min(costs))
        logz.log_tabular('MaximumCost', np.max(costs))
        # In terms of true environment reward of your rolled out trajectory
        # using the MPC controller
        logz.log_tabular('AverageReturn', np.mean(returns))
        logz.log_tabular('StdReturn', np.std(returns))
        logz.log_tabular('MinimumReturn', np.min(returns))
        logz.log_tabular('MaximumReturn', np.max(returns))
        logz.dump_tabular()

        for key in ['observations', 'actions', 'next_observations', 'rewards']:
            paths[key] = np.concatenate([paths[key], newpaths[key]])

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--env_name', type=str, default='Pendulum-v0')
    # Experiment meta-params
    parser.add_argument('--exp_name', type=str, default='mb_mpc')
    parser.add_argument('--seed', type=int, default=3)
    parser.add_argument('--render', action='store_true')
    # Training args
    parser.add_argument('--learning_rate', '-lr', type=float, default=1e-3)
    parser.add_argument('--onpol_iters', '-n', type=int, default=1)
    parser.add_argument('--dyn_iters', '-nd', type=int, default=60)
    parser.add_argument('--batch_size', '-b', type=int, default=512)
    # Data collection
    parser.add_argument('--random_paths', '-r', type=int, default=10)
    parser.add_argument('--onpol_paths', '-d', type=int, default=10)
    parser.add_argument('--simulated_paths', '-sp', type=int, default=1000)
    parser.add_argument('--ep_len', '-ep', type=int, default=1000)
    # Neural network architecture args
    parser.add_argument('--n_layers', '-l', type=int, default=2)
    parser.add_argument('--size', '-s', type=int, default=500)
    # MPC Controller
    parser.add_argument('--mpc_horizon', '-m', type=int, default=15)
    args = parser.parse_args()

    # Set seed
    np.random.seed(args.seed)
    tf.set_random_seed(args.seed)

    # Make data directory if it does not already exist
    if not(os.path.exists('data')):
        os.makedirs('data')
    logdir = args.exp_name + '_' + args.env_name + '_' +time.strftime(
        "%d-%m-%Y_%H-%M-%S")
    logdir = os.path.join('data', logdir)
    if not(os.path.exists(logdir)):
        os.makedirs(logdir)

    # Make env
    if args.env_name == "aircraft":
        env = AircraftEnv()
        cost_fn = aircraft_cost_fn

    if args.env_name == "pendulum":
        env = gym.make("Pendulum-v0")
        cost_fn = pendulum_cost_fn

    train(env=env, 
            cost_fn=cost_fn,
            exp_name=args.exp_name,
            logdir=logdir,
            render=args.render,
            learning_rate=args.learning_rate,
            onpol_iters=args.onpol_iters,
            dynamics_iters=args.dyn_iters,
            batch_size=args.batch_size,
            num_paths_random=args.random_paths, 
            num_paths_onpol=args.onpol_paths, 
            num_simulated_paths=args.simulated_paths,
            env_horizon=args.ep_len, 
            mpc_horizon=args.mpc_horizon,
            n_layers = args.n_layers,
            size=args.size,
            activation=tf.nn.relu,
            output_activation=None,
            )

if __name__ == "__main__":
    main()
